---
title: "TypeScript SDK"
description: "Setting up Langtrace Typescript SDK with OTEL Collector"
---

## Overview

This guide will walk you through the steps to set up Langtrace with OpenTelemetry (OTEL) Collector using the HTTP/JSON protocol in a TypeScript project.

## Configurations

### OTEL Collector

Refer to the OTEL configuration page for instructions on running and configuring the OpenTelemetry Collector with a custom configuration file.

[OTEL Configuration](./otel-configuration)

<Warning> Ensure that you configure the Langtrace API token in the OTEL collector configuration file before proceeding. </Warning>

### TypeScript SDK

To send traces from your TypeScript project to Langtrace, use the following code snippet. Update the API key and endpoint as necessary.

Install the OpenTelemetry HTTP/JSON exporter package:

```bash
npm install @opentelemetry/exporter-trace-otlp-http
```

## Implementation

Initialize the Langtrace SDK with a custom remote exporter that uses the OTLPTraceExporter to send traces to Langtrace Cloud or your self-hosted Langtrace instance.

<AccordionGroup>
  <Accordion title="Example pre-requisite steps for running the code.">

    Create a folder and setup a project
    ```bash
    mkdir langtrace-otel
    cd langtrace-otel
    touch langtrace_otel.js
    npm init -y
    ```

    Edit the `package.json` file and add `"type": "module"` to enable running JS files directly:
    ```json
    {
      ...
      "type": "module",
      ...
    }
    ```

    Install the required packages:
    ```bash
      npm i openai @langtrase/typescript-sdk @opentelemetry/exporter-trace-otlp-http
    ```

    Export the OpenAI key:
    ```bash
    export OPENAI_API_KEY="your-openai-api-key"
    ```

  </Accordion>

</AccordionGroup>

```typescript langtrace_otel.js
import * as Langtrace from "@langtrase/typescript-sdk";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-http";
import OpenAI from "openai";

Langtrace.init({
	batch: false,
	custom_remote_exporter: new OTLPTraceExporter({
		url: "http://localhost:4318/v1/traces",
	}),
	instrumentations: {
		openai: OpenAI,
	},
});

const openai = new OpenAI();

export async function example() {
	const completion = await openai.chat.completions.create({
		model: "gpt-4o-mini",
		messages: [
			{
				role: "system",
				content: "How many states of matter are there?",
			},
		],
	});
	console.log(completion.choices[0]);
}

await example();
```

Run the script:

```bash
node langtrace_otel.js
```

<Info>
	Ignore the following warning message from the Langtrace SDK: 
```bash
No API key provided. Please provide an API key to start sending traces to Langtrace.
```

</Info>

## Conclusion

By following this guide, you will have added OpenTelemetry support to your TypeScript project, enabling you to send traces to Langtrace Cloud or your self-hosted Langtrace instance using the HTTP/JSON protocol.
