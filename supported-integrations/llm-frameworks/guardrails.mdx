---
title: 'Guardrails'
description: 'Learn how to use Langtrace with Guardrails AI for enhanced LLM validation and tracing'
---

# Guardrails

Guardrails AI is a framework for adding validation and safety rails to Large Language Model outputs. Langtrace provides native integration with Guardrails, automatically capturing traces with validator information and metadata to help you monitor and analyze your model's performance and validation results.

## Setup

1. Install Langtrace's SDK and [initialize](/quickstart) the SDK in your code.

2. Install Guardrails SDK and set up your validators according to the [Guardrails documentation](https://www.guardrailsai.com/docs).

3. Initialize Langtrace in your code and implement Guardrails validation:
```python
import os
from langtrace_python_sdk import langtrace
from guardrails import Guard, OnFailAction
from guardrails.hub import ProfanityFree, ToxicLanguage

langtrace.init()

guard = Guard()
guard.name = 'ChatBotGuard'
guard.use_many(
    ProfanityFree(on_fail=OnFailAction.EXCEPTION),
    ToxicLanguage(on_fail="exception"),
    on="output"
)

result = guard(
    messages=[{"role": "user", "content": "Hello, how are you?"}],
    model="gpt-4",
    stream=False,
)
```

## Monitoring Validation Results

Once implemented, Langtrace automatically captures:
- Validator execution and results
- Validation metadata
- Model inputs and outputs
- Validation failures and exceptions

View your traces in the Langtrace dashboard:

![Langtrace Guardrails Trace](/images/integrations/guardrails/langtrace-guardrails-trace1.png)
![Langtrace Guardrails Validation](/images/integrations/guardrails/langtrace-guardrails-trace2.png)

## Resources

- [Getting started with Langtrace](/introduction)
- [Guardrails AI Documentation](https://www.guardrailsai.com/docs)
- [Join our Discord Community](https://discord.langtrace.ai/)
