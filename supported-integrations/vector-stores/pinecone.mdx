---
title: "Pinecone"
description: "Pinecone is a vector database that enables fast and accurate vector search for building AI applications.
It provides the infrastructure for the long-term memory and retrieval needed to develop state-of-the-art AI systems."
---

## Setup

1. Install Langtrace's SDK and [initialize](/quickstart) the SDK in your code.

2. Install packages and setup environment:
   <CodeGroup>
     ```bash Python 
     pip install -U langtrace-python-sdk pinecone-client 
     ```
     ```bash Typescript 
     npm install @pinecone-database/pinecone-client 
     ```
   </CodeGroup>

## Usage

Here's a quick example of how to use Langtrace with Pinecone:

<CodeGroup>
```python Python
import os
os.environ['LANGTRACE_API_KEY'] = "YOUR LANGTRACE_API_KEY"
os.environ['PINECONE_API_KEY'] = "YOUR PINECONE_API_KEY"
 ```
 ```javascript Typescript
 import { init as langtraceInit } from 'langtrase/typescript-sdk';
import { PineconeClient, ServerlessSpec } from '@pinecone-database/pinecone-client';

const langtraceApiKey = "YOUR_LANGTRACE_API_KEY";
const pineconeApiKey = "YOUR_PINECONE_API_KEY";

```
</CodeGroup>


Imports
<CodeGroup>
   ```python Python
   from langtrace_python_sdk import langtrace # Must precede any llm module imports
langtrace.init(api_key = os.environ['LANGTRACE_API_KEY'])
```

```javascript Typescript
 langtraceInit({
  apiKey: langtraceApiKey,
  });

// Step 3: Initialize Pinecone client
const pinecone = new PineconeClient({
  apiKey: pineconeApiKey,
});

```

    </CodeGroup>

Create an Index and upsert some data in Pinecone:

<CodeGroup>
     ```python Python
pc.create_index(
    name="index",
    dimension=8, # Replace with your model dimensions
    metric="euclidean", # Replace with your model metric
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)
index = pc.Index("index")
index.upsert(
  vectors=[
    {"id": "A", "values": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]},
    {"id": "B", "values": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]},
    {"id": "C", "values": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]},
    {"id": "D", "values": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}
  ]
)
 ```
 ```javascript Typescript

// Step 4: Create an Index and upsert some data in Pinecone
(async () => {
await pinecone.createIndex({
name: "index",
dimension: 8, // Replace with your model dimensions
metric: "euclidean", // Replace with your model metric
spec: new ServerlessSpec({
cloud: "aws",
region: "us-east-1",
}),
});

const index = pinecone.Index("index");

await index.upsert({
vectors: [
{ id: "A", values: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] },
{ id: "B", values: [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2] },
{ id: "C", values: [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3] },
{ id: "D", values: [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4] },
],
});
})();

```
</CodeGroup>

You can now view your traces on the Langtrace dashboard:
![traces](/images/pinecone.png)

Want to see more supported methods? Checkout the sample code in the [Langtrace Pinecone Python Example](https://github.com/Scale3-Labs/langtrace-recipes/blob/main/integrations/vector-db/pinecone/starter.ipynb) repository.
