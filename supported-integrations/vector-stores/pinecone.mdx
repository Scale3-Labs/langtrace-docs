---
title: "Pinecone"
description: "Pinecone is a vector database that enables fast and accurate vector search for building AI applications.
It provides the infrastructure for the long-term memory and retrieval needed to develop state-of-the-art AI systems."
---
## Setup

1. Install Langtrace's SDK and [initialize](/quickstart) the SDK in your code.


2. Install packages and setup environment:
<CodeGroup>
    ````bash Python
    pip install -U langtrace-python-sdk pinecone-client
   ````
   ````bash Typescript
   npm install @pinecone-database/pinecone-client
   ````
   </CodeGroup>

## Usage
Here's a quick example of how to use Langtrace with Pinecone:

<CodeGroup>
````bash Python
import os
os.environ['LANGTRACE_API_KEY'] = "YOUR LANGTRACE_API_KEY"
os.environ['PINECONE_API_KEY'] = "YOUR PINECONE_API_KEY"
 ````
 ````bash Typescript
 import { init as langtraceInit } from 'langtrase/typescript-sdk';
import { PineconeClient, ServerlessSpec } from '@pinecone-database/pinecone-client';

const langtraceApiKey = "YOUR_LANGTRACE_API_KEY";
const pineconeApiKey = "YOUR_PINECONE_API_KEY";
 ````
</CodeGroup>


 Imports
<CodeGroup>
    ````bash Python
    from langtrace_python_sdk import langtrace # Must precede any llm module imports
langtrace.init(api_key = os.environ['LANGTRACE_API_KEY'])
````

````bash Typescript
 langtraceInit({
  apiKey: langtraceApiKey,
});

// Step 3: Initialize Pinecone client
const pinecone = new PineconeClient({
  apiKey: pineconeApiKey,
});

````
    </CodeGroup>

Create an Index and upsert some data in Pinecone:
<CodeGroup>
     ```bash Python
pc.create_index(
    name="index",
    dimension=8, # Replace with your model dimensions
    metric="euclidean", # Replace with your model metric
    spec=ServerlessSpec(
        cloud="aws",
        region="us-east-1"
    )
)
index = pc.Index("index")
index.upsert(
  vectors=[
    {"id": "A", "values": [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]},
    {"id": "B", "values": [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]},
    {"id": "C", "values": [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]},
    {"id": "D", "values": [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}
  ]
)
 ```
 ````bash Typescript

// Step 4: Create an Index and upsert some data in Pinecone
(async () => {
  await pinecone.createIndex({
    name: "index",
    dimension: 8, // Replace with your model dimensions
    metric: "euclidean", // Replace with your model metric
    spec: new ServerlessSpec({
      cloud: "aws",
      region: "us-east-1",
    }),
  });

  const index = pinecone.Index("index");

  await index.upsert({
    vectors: [
      { id: "A", values: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1] },
      { id: "B", values: [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2] },
      { id: "C", values: [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3] },
      { id: "D", values: [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4] },
    ],
  });
})();
 ````
</CodeGroup>

You can now view your traces on the Langtrace dashboard:
![traces](/images/pinecone.png)

Want to see more supported methods? Checkout the sample code in the [Langtrace Pinecone Python Example]
(https://github.com/Scale3-Labs/langtrace-recipes/blob/main/integrations/vector-db/pinecone/starter.ipynb) repository.

