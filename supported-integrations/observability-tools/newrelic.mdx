---
title: "New Relic"
description: "Langtrace and New Relic Integration Guide"
---

## Overview

New Relic is a powerful observability platform for monitoring application performance. This guide focuses on integrating Langtrace AI with New Relic for distributed tracing. By leveraging New Relic's capabilities, you can analyze and visualize trace data from Langtrace AI.

## Prerequisites

Before you begin, ensure you have the following:

- A [New Relic](https://newrelic.com/) account
- A [Langtrace AI](http://langtrace.ai/) account

## Setup

### Obtain New Relic API Key

To obtain your New Relic API key:

1. Log in to your [New Relic account](https://one.newrelic.com/)
2. Navigate to the API keys page in your account settings
3. Create a new API key or use an existing one with appropriate permissions

### Set Environment Variables

Set the following environment variables in your terminal or .env file:

```bash
export OTEL_SERVICE_NAME=newrelic-otel-env
export OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net
export OTEL_EXPORTER_OTLP_HEADERS=api-key=your-new-relic-api-key
```

Replace `your-new-relic-api-key` with your actual New Relic API key.

### Install the Instrumentation Library

Install the necessary packages:

```bash
pip install -U opentelemetry-exporter-otlp-proto-http langtrace_python_sdk
```

### Add the custom exporter to your code

Add the following code snippet to your Python application:

<Accordion title="Click to expand the code">

```python
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
import os

# Get environment variables
OTEL_EXPORTER_OTLP_ENDPOINT = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT')
OTEL_EXPORTER_OTLP_HEADERS = os.getenv('OTEL_EXPORTER_OTLP_HEADERS')

# Parse headers
headers = {}
if OTEL_EXPORTER_OTLP_HEADERS:
    key_value = OTEL_EXPORTER_OTLP_HEADERS.split('=', 1)
    if len(key_value) == 2:
        headers[key_value[0]] = key_value[1]

# Add Content-Type header
headers["Content-Type"] = "application/json"

# Configure the OTLP exporter
otlp_exporter = OTLPSpanExporter(
    endpoint=f"{OTEL_EXPORTER_OTLP_ENDPOINT}:443/v1/traces",
    headers=headers
)
```

</Accordion>

### Initialize the Langtrace SDK

Initialize the Langtrace SDK in your application:

```python
from langtrace_python_sdk import langtrace

# Initialize Langtrace with New Relic exporter
langtrace.init(
    custom_remote_exporter=otlp_exporter,
    batch=True
)
```

### Run the Application

With the environment variables set, run your application:

```
opentelemetry-instrument python your_application.py
```

### Verifying the Setup

Once the application is running, you should see traces appearing in your New Relic dashboard. Look for the service name you specified in the `OTEL_SERVICE_NAME` environment variable.

![Elastic](/images/Xnapper-2024-08-16-17.16.09.png)

### Example Application

Here's a simple example of a Python application using Langtrace AI and New Relic. Check out our [Langtrace Recipes](https://github.com/Scale3-Labs/langtrace-recipes/tree/main/integrations/observability) repository for more examples:

<Accordion title="Click to expand the example code">

```python
import os
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from langtrace_python_sdk import langtrace, with_langtrace_root_span
from openai import OpenAI

# Load environment variables (consider using python-dotenv for .env file support)
OTEL_EXPORTER_OTLP_ENDPOINT = os.getenv('OTEL_EXPORTER_OTLP_ENDPOINT')
OTEL_EXPORTER_OTLP_HEADERS = os.getenv('OTEL_EXPORTER_OTLP_HEADERS')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

# Parse headers
headers = dict(item.split("=") for item in OTEL_EXPORTER_OTLP_HEADERS.split(",")) if OTEL_EXPORTER_OTLP_HEADERS else {}

# Configure the OTLP exporter
otlp_exporter = OTLPSpanExporter(
    endpoint=f"{OTEL_EXPORTER_OTLP_ENDPOINT}:443/v1/traces",
    headers={**headers, "Content-Type": "application/json"}
)

# Initialize Langtrace with New Relic exporter
langtrace.init(
    custom_remote_exporter=otlp_exporter,
    batch=True
)

# Initialize OpenAI client
client = OpenAI(api_key=OPENAI_API_KEY)

@with_langtrace_root_span()
def generate_text(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        stream=False
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    result = generate_text("Explain quantum computing in simple terms.")
    print(result)
```

</Accordion>
## Troubleshooting

- **Traces not visible in New Relic**:

  - Ensure all required environment variables are set correctly.
  - Verify that your New Relic API key is valid and has the necessary permissions.
  - Check that the `OTEL_EXPORTER_OTLP_ENDPOINT` is correct for your New Relic account region.

- **Connection Issues**:
  - Ensure your network allows outbound connections to the New Relic endpoint.
  - Verify that the Content-Type header is set to "application/json" in the exporter configuration.

## Additional Resources

- [New Relic OpenTelemetry Documentation](https://docs.newrelic.com/docs/more-integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/)
- [Langtrace AI Documentation](https://docs.langtrace.ai/)
- [OpenTelemetry Python Documentation](https://opentelemetry.io/docs/instrumentation/python/)
