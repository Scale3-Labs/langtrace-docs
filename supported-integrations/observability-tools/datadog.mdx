---
title: "Datadog"
description: "Langtrace and Datadog Integration Guide"
---

## Overview

Datadog is a powerful platform for monitoring the performance of your applications. This guide focuses on integrating Langtrace AI with Datadog APM for distributed tracing of your LLM powered applications.

## Prerequisites

Before you begin, ensure you have the following:

- An [Datadog](https://datadoghq.com/) account

## Setup

In your OpenTelemetry collector setup, make sure you have the collector endpoint environment variable set like this example:

```bash
export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
```

### Initialize the Langtrace SDK

Initialize the Langtrace SDK in your application with your OTLP collector endpoint:

```python
import os
from openai import OpenAI

from langtrace_python_sdk import langtrace
from langtrace_python_sdk.utils.with_root_span import with_langtrace_root_span

from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import \
    OTLPSpanExporter

# Set up the OTLP exporter with the endpoint from your collector
otlp_exporter = OTLPSpanExporter(
    # This should match your collector's OTLP gRPC endpoint
    endpoint=os.environ.get("OTEL_EXPORTER_OTLP_ENDPOINT"),
    insecure=True  # Set to False if using HTTPS
)

# set up langtrace
langtrace.init(custom_remote_exporter=otlp_exporter)

# rest of your code
@with_langtrace_root_span() # This optional annotation is used to create a parent root span for the entire application
def app():
    client = OpenAI(
        api_key="<YOUR_OPENAI_API_KEY>")
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "How many states of matter are there?"
            }
        ],
    )
    print(response.choices[0].message.content)


app()
```

Note: The OTLP collector is setup using a config file in this example. The config file is shown below:

```yaml
receivers:
  otlp:
    protocols:
      http:
        endpoint: "localhost:4318"
      grpc:
        endpoint: "localhost:4317"

processors:
  batch:

exporters:
  datadog:
    api:
      site: "datadoghq.com"
      key: "<DATADOG_API_KEY>"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [datadog]
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [datadog]
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [datadog]
```

### **Run the Application**

With the environment variables set, run your application with OpenTelemetry instrumentation. Use the following command:

```bash
python main.py
```

Note: Make sure your collector is running and configured correctly. To run the collector locally, you can use the following command:

```bash
otelcol-contrib --config otel-collector-config.yaml
```

### **Verifying the Setup**

Once the application is running, you should see traces in your Datadog APM dashboard as shown below:

![Datadog Tracing 1](/images/datadog1.png)

![Datadog Tracing 2](/images/datadog2.png)

![Datadog Tracing 3](/images/datadog3.png)

**Missing Libraries**: Verify that all necessary libraries are installed. If you encounter any errors, reinstall the libraries using the provided commands.

**Additional Resources**

- [Datadog OpenTelemetry Documentation](https://docs.datadoghq.com/opentelemetry/)
